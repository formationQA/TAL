{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-14T23:33:08.261575Z",
     "start_time": "2025-06-14T23:33:07.125136Z"
    }
   },
   "source": [
    "# üìö Corpus Extraction, Nettoyage et √âquilibrage\n",
    "\n",
    "import requests, re, time, os, fitz\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-14T23:33:08.281955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# üìå Config\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "DATE = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "TAILLE_CIBLE = 450"
   ],
   "id": "eb9472e5124487b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# üßΩ Nettoyage & segmentation\n",
    "def nettoyer_et_segmenter(texte, seuil_mots=5):\n",
    "    texte = re.sub(r\"\\[.*?\\]|\\([^)]+\\)|http\\S+|[\\\"*\\[\\]\\{\\}<>\\|\\\\/~^=]\", \"\", str(texte)).strip()\n",
    "    texte = re.sub(r\"\\s+\", \" \", texte)\n",
    "    doc = nlp(texte)\n",
    "    return [sent.text.strip() for sent in doc.sents if len(sent.text.strip().split()) >= seuil_mots]\n"
   ],
   "id": "6bf6a95c3d7209cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üî¥ Reddit\n",
    "def scraper_reddit(url):\n",
    "    print(\"üî¥ Reddit...\")\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        texte = \" \".join(p.get_text() for p in soup.select(\"div.md > p\"))\n",
    "        return [{\"texte\": t, \"registre\": \"familier\", \"date\": DATE} for t in nettoyer_et_segmenter(texte)]\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Reddit:\", e)\n",
    "        return []"
   ],
   "id": "2b5c5515e04cac21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# üü° Wikip√©dia\n",
    "def scraper_wikipedia(urls):\n",
    "    print(\"üü° Wikip√©dia...\")\n",
    "    corpus = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            texte = \" \".join(p.get_text() for p in soup.find_all(\"p\") if len(p.get_text()) > 40)\n",
    "            phrases = nettoyer_et_segmenter(texte)\n",
    "            corpus += [{\"texte\": t, \"registre\": \"courant\", \"date\": DATE} for t in phrases]\n",
    "            print(f\"  ‚úÖ {len(phrases)} phrases de {url}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Wikip√©dia : {url} ‚Üí {e}\")\n",
    "    return corpus\n"
   ],
   "id": "f6a67f743e0aa6bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üîµ PDF\n",
    "def extraire_phrases_pdfs(chemins):\n",
    "    print(\"üîµ Extraction PDF...\")\n",
    "    corpus = []\n",
    "    for path in chemins:\n",
    "        try:\n",
    "            texte = \"\".join(page.get_text() for page in fitz.open(path))\n",
    "            phrases = nettoyer_et_segmenter(texte, seuil_mots=6)\n",
    "            corpus += [{\"texte\": t, \"registre\": \"academique\", \"date\": DATE} for t in phrases]\n",
    "            print(f\"  ‚úÖ {len(phrases)} phrases depuis : {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå PDF : {path} ‚Üí {e}\")\n",
    "    return corpus"
   ],
   "id": "658c5abc0f5e117d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üì• Extraction\n",
    "reddit_url = \"https://old.reddit.com/r/france/comments/1kws0vy/...\"\n",
    "wiki_urls = [\n",
    "    \"https://fr.wikipedia.org/wiki/Technologie\",\n",
    "    \"https://fr.wikipedia.org/wiki/Intelligence_artificielle\"\n",
    "]\n",
    "pdf_paths = [\"../data/academique.pdf\", \"../data/climat.pdf\"]\n",
    "\n",
    "data_all = {\n",
    "    \"familier\": scraper_reddit(reddit_url),\n",
    "    \"courant\": scraper_wikipedia(wiki_urls),\n",
    "    \"academique\": extraire_phrases_pdfs(pdf_paths)\n",
    "}"
   ],
   "id": "88f1aa3add688e33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "corpus_total = []\n",
    "for registre, data in data_all.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty or \"texte\" not in df.columns:\n",
    "        continue\n",
    "    df[\"texte_nettoye\"] = df[\"texte\"].str.lower().str.strip().replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df[df[\"texte_nettoye\"].str.len() >= 20]\n",
    "    df = df.drop_duplicates(subset=\"texte_nettoye\").dropna()\n",
    "    df[\"registre\"] = registre\n",
    "    corpus_total.append(df)\n",
    "    df.to_csv(f\"corpus_{registre}.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"üíæ corpus_{registre}.csv ‚Üí {len(df)} lignes\")\n",
    "\n",
    "# ‚öñÔ∏è Fusion & √©quilibrage\n",
    "df_complet = pd.concat(corpus_total, ignore_index=True)\n",
    "df_equilibre = df_complet.groupby(\"registre\", group_keys=False).apply(\n",
    "    lambda g: g.sample(n=min(len(g), TAILLE_CIBLE), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_equilibre.to_csv(\"corpus_equilibre.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\n‚úÖ corpus_equilibre.csv g√©n√©r√© avec succ√®s !\")\n",
    "print(df_equilibre[\"registre\"].value_counts())"
   ],
   "id": "7c0b5f46a082a99c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# üëÄ Aper√ßu\n",
    "for r in df_equilibre[\"registre\"].unique():\n",
    "    print(f\"\\nüîπ {r.upper()} :\")\n",
    "    print(df_equilibre[df_equilibre[\"registre\"] == r][\"texte_nettoye\"].head(3).to_string(index=False))\n"
   ],
   "id": "6e58aeaec62efc2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
