{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:35:30.095176Z",
     "start_time": "2025-06-14T23:35:30.090786Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "\n",
    "import requests, re, time, os, fitz\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:35:31.204466Z",
     "start_time": "2025-06-14T23:35:30.127315Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 20,
   "source": [
    "#  Config\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "DATE = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "TAILLE_CIBLE = 450"
   ],
   "id": "eb9472e5124487b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:38:46.024524Z",
     "start_time": "2025-06-14T23:38:46.020606Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "\n",
    "#  Nettoyage\n",
    "def nettoyer_text(texte, seuil_mots=5):\n",
    "    texte = re.sub(r\"\\[.*?\\]|\\([^)]+\\)|http\\S+|[\\\"*\\[\\]\\{\\}<>\\|\\\\/~^=]\", \"\", str(texte)).strip()\n",
    "    texte = re.sub(r\"\\s+\", \" \", texte)\n",
    "    doc = nlp(texte)\n",
    "    return [sent.text.strip() for sent in doc.sents if len(sent.text.strip().split()) >= seuil_mots]\n"
   ],
   "id": "6bf6a95c3d7209cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:35:31.303646Z",
     "start_time": "2025-06-14T23:35:31.296452Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 22,
   "source": [
    "# Reddit\n",
    "def scraper_reddit(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        texte = \" \".join(p.get_text() for p in soup.select(\"div.md > p\"))\n",
    "        return [{\"texte\": t, \"registre\": \"familier\", \"date\": DATE} for t in nettoyer_text(texte)]\n",
    "    except Exception as e:\n",
    "        print(\" Reddit:\", e)\n",
    "        return []"
   ],
   "id": "2b5c5515e04cac21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:35:31.368286Z",
     "start_time": "2025-06-14T23:35:31.361831Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 23,
   "source": [
    "\n",
    "#  Wikip√©dia\n",
    "def scraper_wikipedia(urls):\n",
    "    corpus = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            texte = \" \".join(p.get_text() for p in soup.find_all(\"p\") if len(p.get_text()) > 40)\n",
    "            phrases = nettoyer_text(texte)\n",
    "            corpus += [{\"texte\": t, \"registre\": \"courant\", \"date\": DATE} for t in phrases]\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Wikip√©dia : {url} ‚Üí {e}\")\n",
    "    return corpus\n"
   ],
   "id": "f6a67f743e0aa6bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:35:31.434842Z",
     "start_time": "2025-06-14T23:35:31.429137Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 24,
   "source": [
    "#  PDF\n",
    "def extraire_phrases_pdfs(chemins):\n",
    "    corpus = []\n",
    "    for pathpdf in chemins:\n",
    "        try:\n",
    "            texte = \"\".join(page.get_text() for page in fitz.open(pathpdf))\n",
    "            phrases = nettoyer_text(texte, seuil_mots=5)\n",
    "            corpus += [{\"texte\": t, \"registre\": \"academique\", \"date\": DATE} for t in phrases]\n",
    "        except Exception as e:\n",
    "            print(f\"ERR  : {pathpdf} ‚Üí {e}\")\n",
    "    return corpus"
   ],
   "id": "658c5abc0f5e117d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:36:16.625847Z",
     "start_time": "2025-06-14T23:35:31.494531Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Reddit...\n",
      "üü° Wikip√©dia...\n",
      "  ‚úÖ 111 phrases de https://fr.wikipedia.org/wiki/Technologie\n",
      "  ‚úÖ 527 phrases de https://fr.wikipedia.org/wiki/Intelligence_artificielle\n",
      "üîµ Extraction PDF...\n",
      "  ‚úÖ 137 phrases depuis : ../databrut/academique.pdf\n",
      "  ‚úÖ 4872 phrases depuis : ../databrut/climat.pdf\n"
     ]
    }
   ],
   "execution_count": 25,
   "source": [
    "#  Les chemins\n",
    "reddit_url = \"https://old.reddit.com/r/france/comments/1kws0vy/...\"\n",
    "wiki_urls = [\n",
    "    \"https://fr.wikipedia.org/wiki/Technologie\",\n",
    "    \"https://fr.wikipedia.org/wiki/Intelligence_artificielle\"\n",
    "]\n",
    "pdf_paths = [\"../databrut/academique.pdf\", \"../databrut/climat.pdf\"]\n",
    "\n",
    "data_all = {\n",
    "    \"familier\": scraper_reddit(reddit_url),\n",
    "    \"courant\": scraper_wikipedia(wiki_urls),\n",
    "    \"academique\": extraire_phrases_pdfs(pdf_paths)\n",
    "}"
   ],
   "id": "88f1aa3add688e33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T23:36:16.930575Z",
     "start_time": "2025-06-14T23:36:16.665491Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ corpus_familier.csv ‚Üí 437 lignes\n",
      "üíæ corpus_courant.csv ‚Üí 638 lignes\n",
      "üíæ corpus_academique.csv ‚Üí 4824 lignes\n",
      "\n",
      "‚úÖ corpus_equilibre.csv g√©n√©r√© avec succ√®s !\n",
      "registre\n",
      "academique    450\n",
      "courant       450\n",
      "familier      437\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_131845/3351342430.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_equilibre = df_complet.groupby(\"registre\", group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 26,
   "source": [
    "corpus_total = []\n",
    "for registre, data in data_all.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty or \"texte\" not in df.columns:\n",
    "        continue\n",
    "    df[\"texte_nettoye\"] = df[\"texte\"].str.lower().str.strip().replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df[df[\"texte_nettoye\"].str.len() >= 20]\n",
    "    df = df.drop_duplicates(subset=\"texte_nettoye\").dropna()\n",
    "    df[\"registre\"] = registre\n",
    "    corpus_total.append(df)\n",
    "    df.to_csv(f\"corpus_{registre}.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "df_complet = pd.concat(corpus_total, ignore_index=True)\n",
    "df_equilibre = df_complet.groupby(\"registre\", group_keys=False).apply(\n",
    "    lambda g: g.sample(n=min(len(g), TAILLE_CIBLE), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_equilibre.to_csv(\"corpus_equilibre.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\n‚úÖ corpus_equilibre.csv g√©n√©r√© avec succ√®s !\")"
   ],
   "id": "7c0b5f46a082a99c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
